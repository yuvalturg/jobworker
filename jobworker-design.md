---
authors: Yuval Turgeman (yuvalt@gmail.com)
state: draft
---

# Job Worker Service

## What

Implement a job worker service that is capable to execute isolated and resource-limited Linux processes on the server side. The service will expose an API for starting, stopping, querying status, and streaming output for each process.

## Why

This is an exercise that encompasses various concepts in a modern golang development project, including

- Secure client-server development using gRPC
- Role based access control
- API definition with protobuf
- Unerstanding of a Linux OS, and familiry with advanced subsystems (cgroups, namespaces)
- Asynchronous development using goroutines and channels

## Requirements

The project will be split up to three components

### Job Manager Library
The library will be incharge of the heavy lifting of the entire process management.  It will handle both the bookkeeping for the jobs and the actual lifecycle of the underlying processes, including resource limitations using cgroups' cpu, memory and io controllers, and isolation using pid, network, and mount namespaces.  It will provide the following basic functions:
- **StartJob**: execute a new job
- **StopJob**: kill a running job
- **QueryJob**: fetch the current status of a job
- **StreamJob**: stream the standard output and error from the begining of the execution


### Server
The jobworker server will use gRPC, and its API will be defined with protobuf and will include relevant messages for start/stop/query/stream.
The server will use mTLS, with certificates generated by openssl.


### Client
The jobworker client will be able to connect to the gRPC server to start/stop/query/stream jobs.

## Job Manager Library
The library will introduce a basic job struct in the following manner:
```
type job struct {
    jobId string
    args []string
    logpath string
    cancel context.CancelFunc
}
```

### StartJob
#### Input: `command arguments`
#### Output: `jobId (UUID)`, `error`
#### Process:
- Command arguments will include the command and its arguments, for example ["ls", "-lrt", "/tmp"], similar to python's subprocess.Popen().  Just to clarify, we will not be using bash for execution.  We will search for `args[0]` before execution using `exec.LookPath`, and use that as the command path.
- A new job with a unique `jobId` will be issued by the manager.  We will use UUID for `jobId`.
- The required command arguments will be copied to the job.
- A new cgroup with the relevant resource limitations will be created under `/sys/fs/cgroup/$jobId`.
- A unique logfile will be created under `/var/log/jobworker/$jobId.log`.
- A context cancel function will be registered in case we would like to kill the process.
- The job will be kept inside the manager's jobDb.
- The command will be executed and its stdout and stderr will be redirected to `$logfile`.
- When setting up the process for execution, the library will open the cgroup fd, and assign it to exec.Cmd.SysProcAttr.  It will also set the relevant clone flags in order to setup namespace isolation, this way the process will be executed immediately in its cgroup and namespace definitions, instead of having to execute a placeholder "pause()" binary and attach the real process to its cgroup and namespace.
- The underlying process will have to be monitored in order to call Wait() whenever the process exits in order to clean its resources.
- The manager will maintain the following job statuses:
    1. JobQueued: A job was requested and not yet started
    2. JobStartFailed:  The process didn't start (perhaps the command doesn't exist)
    3. JobRunning: The process is running
    4. JobFinished: The process ended, either gracefully or by a kill()

#### Cgroups
As mentioned before we will create a cgroup-per-process, we will use the cpu, memory and io controllers.  The actions that we'll take in order to prepare this are:
1. Create a new cgroup `/sys/fs/cgroup/$jobId`.
2. Ensure we have the relevant controllers set up by adding them to `/sys/fs/cgroup/cgroup.subtree_control`.
3. Set predefined values for io.max, cpu.max, and memory.max.  For io.max the device that will be selected will be the devices mounted on /.  We will search for the device under /proc/mounts and extract the major and minor using stat().
4. Get a filedescriptor for the cgroup using open($cgrou_path, unix.O_PATH)
5. Assign the cgroup file descriptor to the command's `SysProcAttr`, and set the UseCgroupFD flag to true.

#### Namespace Isolation
For this project, we will only use the relevant cloneflags when executing the process.  This is not enough for a full container runtime.  For a real isolation, we need several steps that involve 2 processes, a parent and a child -
1. The parent needs to set up a directory which will serve as the new rootfs for the container.
2. The parent will need to setup networking devices, probably using a bridge in which one end of the bridge will be inside the namespace that we are creating, and the other end will be on the host.
3. The parent will need to create the namespace using the `clone` syscall
4. The child will be the leader of a new process group using setpgid, in order to clean up any subprocesses that are executed afterwards.
5. The child process will then have to mount a new /proc filesystem.  This will also detach the child from the host's process list.  This will make our process pid=1.
6. The child process will need to pivot_root to the new filesystem to detach itself from the host's rootfs, in a similar way that linux boot with a ramdisk.

**Note**: In Go, we do not have direct access to the clone syscall, but instead, we can set the clone flags as part of the binary execution as follows:
```
        cmd := exec.Command("/bin/bash")
        cmd.SysProcAttr = &syscall.SysProcAttr{
                Cloneflags: syscall.CLONE_NEWPID|syscall.CLONE_NEWNS|syscall.CLONE_NEWNET,
                Setpgid: true,
                UseCgroupFD: true,
			    CgroupFD: cgroupFd,
        }
```
This presents a challange, so we will probably need to execute 2 binaries, a wrapper binary that will setup the namespaces, and then execve the real binary, replacing the wrapping process.  The running process, after mounting its new /proc, will be pid 1.


### StopJob
#### Input: `jobId (UUID)`
#### Output: `jobStatus`, `error`
#### Process:
- Lookup the job in the jobDB.
- If the job is found, access its `cancelFunc`, and cancel the job's context. Once the context gets cancelled, the process will be killed.
- If the job is not found, return a NotFoundError with a NotFound status.


### QueryJob
#### Input: `jobId (UUID)`
#### Output: `jobStatus`, `error`
#### Process:
- Lookup the job in the jobDB.
- If the job is found, return its `jobStatus` field, `processId` and `exitStatus`
- If the job is not found, return a NotFoundError with a NotFound status.


### StreamJob
#### Input: `jobId (UUID), output channel`
#### Output: `error`
#### Process:
- Lookup the job in the jobDB.
- Streaming will be done using inotify. The manager will hold an inotify watcher file descriptor, and if the job's lookup is successful, the manager will add the logfile's file descriptor to the watcher.  Whenever a IN_MODIFY event is received on the watcher, we will read the data and stream it to a channel that will eventually be received by the client.
- Each chunk of data that is read will be sent to the given output channel, which shuold be processed by the caller (the server in our case).
- To stop streaming, we will remove the logfile's descriptor from inotify and close the channel -  This can be done by the caller whenever it detects a stream context is done.
- Streaming will stop automatically if there's no more data to read and the job status is set to stopped.
- If the job is not found, return a NotFoundError.


### Notes
- The library will not persist the db it uses for bookkeeping, but instead it will hold the job information in memory.  This means, that whenever the server exists, eventhough the log files will still be present in the logdir, their data will not be visible with the system.
- The manager will be using a cgroup-per-job approach.
- The manager will be using [cgroups v2](https://docs.kernel.org/admin-guide/cgroup-v2.html)


## Server
- The server will expose a gRPC API to StartJob/StopJob/QueryJob/StreamJob.
- The server will use mTLS, so it will receive the client side certificate and on each rpc call the server will inspect its peer's subject, which will hold the user name as a CommonName.  See examples how to generate client certificates in the Security section.
- The server will maintain a "db" of users (in memory for the prototype), that are allowed to execute commands.  Additionally, it will maintain ownership model in which each user can access their own jobs.  It will take the following actions on each request:
    1. Extract .Subject.CommonName from the client certificate.
    2. **StartJob**: The auth system will add an entry `jobId -> userId` in the auth db.
    3. **StopJob/StreamJob/QueryJob**: The auth system will query the auth db and check if the job may be accessed by the user.
- If the request get denied according to the rules mentioned above, the intercepting function will return a PermissionDenied and the request will not be forwarded to the manager library.


- The protobuf definition will look something like
```
syntax = "proto3";
option go_package = "jobworker/pkg/api";

package jobworker;

service JobWorker {
    rpc StartJob (StartJobRequest) returns (JobResponse);
    rpc StopJob (JobRequest) returns (JobResponse);
    rpc QueryJob (JobRequest) returns (JobResponse);
    rpc StreamJob (JobRequest) returns (stream StreamJobResponse);
}

enum JobStatus {
    JOB_STATUS_UNSPECIFIED = 0;
    JOB_STATUS_QUEUED = 1;
    JOB_STATUS_FAILED_TO_START = 2;
    JOB_STATUS_RUNNING = 3;
    JOB_STATUS_STOPPED = 4;
    JOB_STATUS_NOT_FOUND = 5;
}

message StartJobRequest {
    repeated string arguments = 1;
}

message JobRequest {
    string job_id = 1;
}

message JobResponse {
    string job_id = 1;
    int32 pid = 2;
    int32 exit_code = 3;
    JobStatus status = 4;
}

message StreamJobResponse {
    bytes message = 1;
}
```

## Client
- The client will be a modern cli binary that is built from the following parts:
    - A subcommand that can be any of the following `start` `stop` `status` `stream`
    - A common set of flags for the client initialization
        - ca-path: path for the ca certificate
        - client-key-path: path for the client key
        - client-cert-path: path for the client certificate
        - server-address: the address of the job worker server
    - A specific set of flags for each type of subcommand

### Usage Examples

- Starting a long running job
```
$ ./jobworkerclient \
    --server-address localhost:5555 \
    --ca-path ./certs/ca.crt \
    --client-key-path ./certs/client.key \
    --client-cert-path ./certs/client.crt \
    start \
    -- bash -c "while :; do date; sleep 5; done"

Started JobId=8fa4b245-749f-4579-bff4-daf34056761a Status=Running
```
- Querying status of a job
```
$ ./jobworkerclient query 8fa4b245-749f-4579-bff4-daf34056761a

JobId=8fa4b245-749f-4579-bff4-daf34056761a Status=Running
```

- Stopping a job
```
$ ./jobworkerclient stop 8fa4b245-749f-4579-bff4-daf34056761a

JobId=8fa4b245-749f-4579-bff4-daf34056761a Status=Stopped
```

- Streaming a job
```
$ ./jobworkerclient stream 8fa4b245-749f-4579-bff4-daf34056761a

Thu Mar 28 06:27:01 PM EDT 2024
Thu Mar 28 06:27:06 PM EDT 2024
Thu Mar 28 06:27:11 PM EDT 2024
Thu Mar 28 06:27:16 PM EDT 2024
...
```
To stop the stream, we'll need to send the client a SIGINT (Ctrl-C), we can trap it, gracefully close the stream and exit.

## High Availability
This is a prototype, and as such it will not be highly available.  In fact, the jobs are kept in memory, so whenever the server crashes the list of jobs will be gone.  In order to achieve highly availablity we will need to take several measures:
1. Use a database in order to persist the job list.  The database should be replicated.
2. We should have multiple running services behind a load balancer.
3. The logfiles should also be placed in a known location and replicated, preferably across different regions.
4. Run multiple server controllers on the same host that are aware of each other, this way if one of them crashes, the other will be able to detect this, and continue monitoring the process ids that are currently being executed on the host.

We can achieve this by using kubernetes Deployment with multiple replicas, that are accessed using a kubernetes Service.  The logfiles can be saved either on the cluster with PersistentVoumes or on a remote object storage like S3.

## Security and Privacy
- The cipher suite for TLS13 will use TLS_AES_128_GCM_SHA256
- The system uses mTLS, so both the server and the client need a set of certificates.  For this implementation, we will generate all relevant cerficates using openssl - client, server and CA, and save them all under a single directory.  For production code we should probably use a real CA.  For this project, something like this should work.
```
openssl req -new -x509 -days 365 -keyout ca.key -out ca.crt -subj "/CN=MyCA"

openssl req -new -newkey rsa:2048 -nodes -keyout server.key -out server.csr -subj "/CN=localhost"
echo "subjectAltName = DNS:localhost" > server.ext
openssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out server.crt -days 365 -extfile server.ext

openssl req -new -newkey rsa:2048 -nodes -keyout client.key -out client.csr -subj "/CN=Client"
openssl x509 -req -in client.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out client.crt -days 365

```
- For the stream command, both stdout and stderr will be visible to the client, so there's a privacy consideration here.  For example, a user requests to execute a script that looks like this:
```
#!/bin/bash -x
pass=$(aws ecr get-login-password)
```
Since we are streaming stderr, the user's password will be visible.
- **Important** - In order to create cgroups, without having to alter the system configuration, the server will run as root.
